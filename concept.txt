

Python-Module needed: tqdm colorama
Python 3.5 or higher needed!


Unicode-Filenames makes issues in all steps! hash-creation does not support it, recompressors also not.


Same functionality as the python-script could be done with bash-scripts:
https://web.archive.org/web/20151228095434/http://superuser.com/questions/862323/offline-incremental-backup-via-thumbdrive
	Here's 2 "main" bash lines I've used, the first just to md5 all files to be backed up, then sort & zip the list. Restricting by date could be useful here:
		find . -type f -print0|xargs -0 md5sum|sort -k 1.35|gzip -9 > $newmd5
	And to get new files to add:
		zdiff $newmd5 $oldmd5|sort -k 1.37|sed -n '/^</ p'|cut -c 39- > new.txt
	Could grep "FAILED open" from a md5sum -c of the old md5 info file to find deleted files, or different diff result parsing.
https://github.com/miekg/rdup
https://stackoverflow.com/questions/17334014/move-files-to-directories-based-on-extension
	find . -name '*mp3' -or -name '*ogg' -print | xargs -J% mv % ../../Music
	find . -name '*mp3' -or -name '*ogg' -exec mv {} ../Music \;
https://github.com/shenwei356/rush - A cross-platform command-line tool for executing jobs in parallel




How to make windows command line arguments unicode:
- https://docs.microsoft.com/en-us/cpp/c-runtime-library/reference/fopen-wfopen?view=vs-2019
- http://zetcode.com/gui/winapi/main/


Preprocessors for Files:
- Packing JPegs: https://github.com/dropbox/lepton   in slow-mode is better than google-lib

		commit 24f3bbc9b31c4041d0d1caf71f3d16354379d4f3 (grafted, HEAD -> master, origin/master, origin/HEAD)
		Author: Daniel <danielrh@dropbox.com>
		Date:   Fri May 17 01:16:25 2019 -0700

		    Merge pull request #119 from a4lg/fix-automake
		    
		    make automake not to install test_invariants

- https://github.com/schnaader/precomp-cpp

Checksumming:
https://github.com/cmuratori/meow_hash/blob/master/meow_hash_x64_aesni.h

		commit 70e01b40ee0de82017307b994ed26c457541e250 (grafted, HEAD -> master, origin/master, origin/HEAD)
		Author: Casey Muratori <casey@mollyrocket.com>
		Date:   Sat May 25 00:25:29 2019 -0700

		    Merge pull request #46 from cmuratori/v0.5
		    
		    Starting code for v0.5



zpaq:
	# -m5 is dead slow :/
	# -m4 is much faster already and still better than lzma2 from precomp-cpp

	#-rwxr-xr-x 1 devenv root    2036046 Jun 13 15:49 pixelwp2.png*
	#-rw-rw-r-- 1 devenv devenv 12543751 Jun 13 16:55 pixelwp2.pcf
	#-rw-rw-r-- 1 devenv devenv  1726904 Jun 13 16:54 pixelwp2.pcf_LZMA
	#-rw-rw-r-- 1 devenv devenv  1439668 Jun 13 16:56 test.zpaq (-m5) (50sec)
	#-rw-rw-r-- 1 devenv devenv  1517409 Jun 13 16:59 test.zpaq (-m4) (13sec)



Workflow:

Start by creating a database and compressed file of a specific input folder by "creating"
an archive and it's database with the following parameters:
	-c -i path_to_input -o path_to_output -d path_to_db -t path_to_compiled_tools
Here inputs is your source of files to backup (for example the folder with all pictures), 
path to output is were the compressed output and the filelist of this backup-run is
created and which can be stored to backup-media (dvd, tape, ...). This output-folder can
be deleted after it is stored to another media. The db should not be
created in the output folder, but instead in a folder stored on the harddisc as reference
for future runs. In this database the cummulative file status is stored.

Now changes can be done to the input folder to reflect working on the files (changing files
and adding new ones).

To create the next archive update set rerun the ctrlcenter with the arguments from above, but
use a different output-folder (or use the same as above but with cleared content). The program
will check, based on the knowledge of the database, if files were changed or added. These files
will be compressed (and recompressed) and only these changed files will be stored in the new
folder.

Deleted files will stay in the database file so if you read them to the original path nothing
additional will be updated. If you add the original files to a different path the files will
be recompressed and stored at the different path! (No deduplication).

In case that the database is corrupt, gone or something else happened you can recreate it by
merging the hashfiles to the database, start by removing the current database file and merge
the hashfiles from all backups into the database to get a up-to-date database again.
